{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aef9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60a160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_english_alphabet=pickle.load(open('vocab_tools/index_to_english_alphabet.pickle', 'rb'))\n",
    "index_to_hindi_alphabet=pickle.load(open('vocab_tools/index_to_hindi_alphabet.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bc19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_alphabet_to_index=pickle.load(open('vocab_tools/hindi_alphabet_to_index.pickle', 'rb')) \n",
    "english_alphabet_to_index=pickle.load(open('vocab_tools/english_alphabet_to_index.pickle', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('simple_data/X_train.npy')\n",
    "X_valid=np.load('simple_data/X_val.npy')\n",
    "\n",
    "y_train=np.load('simple_data/y_train.npy')\n",
    "y_valid=np.load('simple_data/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bb32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e291758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eng_Hind_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, in_file, out_file, root_dir='simple_data',device='cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.input = torch.tensor(np.load(root_dir+'/'+in_file))\n",
    "        self.output = torch.tensor(np.load(root_dir+'/'+out_file))\n",
    "        \n",
    "        assert(len(self.input)==len(self.output),\"Error: I/O Lengths must be same\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        X=self.input[idx]\n",
    "        X=X.to(device)\n",
    "        y=self.output[idx]\n",
    "        y=y.to(device)\n",
    "        \n",
    "\n",
    "\n",
    "        sample = {'input': X, 'output': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2d17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d715496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=Eng_Hind_Dataset(\"X_train.npy\",\"y_train.npy\",device=device)\n",
    "val_data=Eng_Hind_Dataset(\"X_val.npy\",\"y_val.npy\",device=device)\n",
    "test_data=Eng_Hind_Dataset(\"X_test.npy\",\"y_test.npy\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b576ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53944bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_data, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d97754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e383a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_type(mode:str='rnn'):\n",
    "    mode=mode.lower()\n",
    "    if mode == 'rnn':\n",
    "        return nn.RNN\n",
    "    elif mode =='gru':\n",
    "        return nn.GRU\n",
    "    else:\n",
    "        return nn.LSTM\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e094b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_dim : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, embed_size,padding_idx=english_alphabet_to_index['.'])\n",
    "\n",
    "        #creating LSTM/GRU/RNN cell\n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_batch) # [sent len, batch size, emb dim]\n",
    "        #print('encoder embd',embedded.shape)\n",
    "        \n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            outputs, (hidden, cell) = self.cell(embedded)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded)\n",
    "            cell=outputs\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761debd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, output_size, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embed_size,padding_idx=hindi_alphabet_to_index['.'])\n",
    "        \n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        self.out = nn.Linear(hid_size, output_size)\n",
    "        \n",
    "        self.output_size=output_size\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "\n",
    "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trg : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(trg.unsqueeze(1))\n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            #print('decoder embed',embedded.shape)\n",
    "            outputs, (hidden, cell) = self.cell(embedded, (hidden, cell))\n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded, hidden)\n",
    "            cell=hidden\n",
    "        prediction = self.out(outputs.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d74ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6329340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b679e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e441d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, source_batch, target_batch, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        batch_size,max_len  = target_batch.shape\n",
    "        #print(max_len,batch_size)\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        #print(target_vocab_size)\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(self.device)\n",
    "#         outputs = torch.zeros( batch_size,max_len, target_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(source_batch)       \n",
    "\n",
    "        trg = target_batch[:,0]\n",
    "#         trg = target_batch[0]\n",
    "        #print('target[0]>',trg)\n",
    "        #print('tbs',target_batch.shape)\n",
    "        #print('trg',trg.shape)\n",
    "        for i in range(1, max_len):\n",
    "            #print('i,hid,cel,',i, hidden.shape,cell.shape)\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            #print('pred',prediction.shape)\n",
    "            #print('outputs',outputs.shape)\n",
    "            \n",
    "            \n",
    "            outputs[i] = prediction\n",
    "            #print('pred',prediction.shape)\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = target_batch[:,i]\n",
    "                #print(1, trg.shape)\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "                #print(2, trg.shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87facdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    seq2seq.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch['input'], batch['output'])\n",
    "        batch_label=batch['output'].transpose(0,1)\n",
    "        \n",
    "        #print('outputs',outputs.shape)\n",
    "        #print('outputsbatchs',batch['output'].shape)\n",
    "        #print('batch_label',batch_label.shape)\n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "        #print('wow_preds',predicted.shape)\n",
    "        \n",
    "        outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch_label.reshape(-1)\n",
    "        \n",
    "#         #print('shape output_flat',outputs_flatten.shape)\n",
    "        #print('shape output_flat',outputs_flatten)\n",
    "        #print('shape trg_flatten',trg_flatten)\n",
    "\n",
    "        trg_flatten.requires_grad=False\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if torch.all(torch.eq(batch['output'].transpose(0,1),predicted)):\n",
    "            correct+=1\n",
    "        #print('shape output_flat',outputs_flatten.shape)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct_char += torch.sum(predicted == batch['output'].transpose(0,1)).item()\n",
    "        tot_char += batch['output'].numel()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), correct/len(iterator),correct_char/tot_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a950559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            outputs = seq2seq(batch['input'], batch['output'],teacher_forcing_ratio=0)\n",
    "            batch_label=batch['output'].transpose(0,1)\n",
    "\n",
    "            #print('outputs',outputs.shape)\n",
    "            #print('outputsbatchs',batch['output'].shape)\n",
    "            #print('batch_label',batch_label.shape)\n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            #print('wow_preds',predicted.shape)\n",
    "\n",
    "            outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch_label.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "\n",
    "\n",
    "            if torch.all(torch.eq(batch['output'].transpose(0,1),predicted)):\n",
    "                correct+=1\n",
    "\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#         for batch in iterator:\n",
    "#             # turn off teacher forcing\n",
    "#             outputs = seq2seq(batch['input'], batch['output'], teacher_forcing_ratio=0)\n",
    "#             _, predicted = torch.max(outputs, dim=2)\n",
    "            \n",
    "\n",
    "#             # trg = [trg sent len, batch size]\n",
    "#             # output = [trg sent len, batch size, output dim]\n",
    "#             outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "#             trg_flatten = batch['output'].view(-1)\n",
    "#             loss = criterion(outputs_flatten, trg_flatten)\n",
    "            \n",
    "#             for jik in range(len(batch['output'])):\n",
    "#                 if torch.all(torch.eq(batch['output'][jik],predicted)):\n",
    "#                     correct+=1\n",
    "                    \n",
    "            correct_char += torch.sum(predicted == batch['output'].transpose(0,1)).item()\n",
    "            tot_char += batch['output'].numel()\n",
    "        \n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), correct/len(iterator),correct_char/tot_char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "718d4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    e_time = end_time - start_time\n",
    "    mins = e_time // 60\n",
    "    secs = e_time%60\n",
    "    return mins, secs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eb08c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d74d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd5bae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_model(train_iterator,valid_iterator,N_EPOCHS=20):\n",
    "    E=Encoder(30,128,128,2,'lstm',0.2,False)\n",
    "    E=E.to(device)\n",
    "    D=Decoder(68,128,128,2,'lstm',0.2,False)\n",
    "    D=D.to(device)\n",
    "    S=Seq2Seq(E,D,device)\n",
    "    S.to(device)    \n",
    "    print(f'The model has {count_params(S):,} trainable parameters')\n",
    "    \n",
    "    optimizer = optim.Adam(S.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=hindi_alphabet_to_index['.'])\n",
    "    criterion=criterion.to(device)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "#     return S\n",
    "\n",
    "    for epoch in range(N_EPOCHS):    \n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc,train_stuff = train(S, train_iterator, optimizer, criterion)\n",
    "        valid_loss,valid_acc,val_stuff = evaluate(S, valid_iterator, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(S.state_dict(), 'model1.pt')\n",
    "\n",
    "        # it's easier to see a change in perplexity between epoch as it's an exponential\n",
    "        # of the loss, hence the scale of the measure is much bigger\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        print(f'\\t Relaxed Train. Acc: {train_stuff*100:.2f}% | Relaxed Val. Acc: {val_stuff*100:.2f}%')\n",
    "        \n",
    "    return S\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1049df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 549,700 trainable parameters\n",
      "Epoch: 01 | Time: 1.0m 17.69s\n",
      "\t Train Loss: 2.709 | Train Acc: 0.00%\n",
      "\t Val. Loss: 2.063 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 12.70% | Relaxed Val. Acc: 15.55%\n",
      "Epoch: 02 | Time: 0.0m 58.00s\n",
      "\t Train Loss: 1.653 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.668 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 20.62% | Relaxed Val. Acc: 18.35%\n",
      "Epoch: 03 | Time: 0.0m 30.19s\n",
      "\t Train Loss: 1.396 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.574 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 22.95% | Relaxed Val. Acc: 19.35%\n",
      "Epoch: 04 | Time: 0.0m 30.19s\n",
      "\t Train Loss: 1.281 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.522 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 24.04% | Relaxed Val. Acc: 19.77%\n",
      "Epoch: 05 | Time: 0.0m 30.19s\n",
      "\t Train Loss: 1.218 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.496 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 24.66% | Relaxed Val. Acc: 20.07%\n",
      "Epoch: 06 | Time: 0.0m 30.20s\n",
      "\t Train Loss: 1.171 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.471 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 25.14% | Relaxed Val. Acc: 20.29%\n",
      "Epoch: 07 | Time: 0.0m 30.20s\n",
      "\t Train Loss: 1.134 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.469 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 25.54% | Relaxed Val. Acc: 20.39%\n",
      "Epoch: 08 | Time: 0.0m 30.19s\n",
      "\t Train Loss: 1.104 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.452 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 25.81% | Relaxed Val. Acc: 20.77%\n",
      "Epoch: 09 | Time: 0.0m 30.20s\n",
      "\t Train Loss: 1.085 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.441 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 26.03% | Relaxed Val. Acc: 20.75%\n",
      "Epoch: 10 | Time: 0.0m 30.20s\n",
      "\t Train Loss: 1.058 | Train Acc: 0.00%\n",
      "\t Val. Loss: 1.425 |  Val. Acc: 0.00%\n",
      "\t Relaxed Train. Acc: 26.29% | Relaxed Val. Acc: 20.93%\n"
     ]
    }
   ],
   "source": [
    "train_iterator=train_dataloader\n",
    "valid_iterator=val_dataloader\n",
    "SS=make_model(train_iterator,valid_iterator,N_EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5468c3f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torcaaah' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example\u001b[38;5;241m=\u001b[39m\u001b[43mtorcaaah\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torcaaah' is not defined"
     ]
    }
   ],
   "source": [
    "example=torcaaah.randn(16,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b88d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.transpose(1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[[1,0],\n",
    "                 [2,30]],[[6,7],\n",
    "                 [8,9]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d59aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b25dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da09392",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=torch.randn(30,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=aa.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea020aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.reshape(-1)==aa.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46376623",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ignore the pcriterion = nn.CrossEntropyLoss(adding index when calculating the loss\n",
    "# PAD_IDX = target.vocab.stoi['<pad>']\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ac4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0512bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bya=SS(a['input'],a['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b03dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5071c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 16, 68])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bya.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea89dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb3f823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_torchies(torchie1,index_toalp):\n",
    "    torchie=torchie1.cpu().numpy()\n",
    "    return word_from_vecs(torchie,index_toalp,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b064da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_batch(batch):\n",
    "    wordlet=[]\n",
    "    for i in range(len(batch)):\n",
    "        wordlet.append(word_from_torchies(batch[i],index_to_hindi_alphabet))\n",
    "    return wordlet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8373ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Word\n"
     ]
    }
   ],
   "source": [
    "word_from_torchies(bya.argmax(2)[3],index_to_hindi_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c28adf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ङ'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_hindi_alphabet[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e70b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrd(stuff):\n",
    "    asa=[]\n",
    "    for k in stuff.cpu().numpy():\n",
    "        asa.append(index_to_hindi_alphabet[k])\n",
    "    return \"\".join(asa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93336ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<नंदािषारिकर्तायय>>>>>>>>>>>>>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_wrd(bya.transpose(0,1).argmax(2)[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06334d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13,  7,  9,  4, 48,  9, 11,  4, 10, 11,  6, 14,  9,  8,  8,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bya.transpose(0,1).argmax(2)[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb2cde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\n",
      "0\n",
      "कंटैक्टलेस --- <कंटेक्टे्स>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "1\n",
      "फेडिकल --- <फेडिकल>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "2\n",
      "सुरों --- <सर्सं>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "3\n",
      "प्रदन्त --- <प्रदा्त>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "4\n",
      "दीवारें --- <दीवारें>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "5\n",
      "जलेगी --- <जलेगी>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "6\n",
      "विल्वक --- <विलववक>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "7\n",
      "नाजुली --- <नजुुली>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "8\n",
      "अशोकधाम --- <अशोकधाम>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "9\n",
      "माईसाहेब --- <मैसाहहेब>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "10\n",
      "नंदकिशोरभारतीय --- <नंदािषारिकर्तायय>>>>>>>>>>>>>\n",
      "................\n",
      "11\n",
      "सम्राटाची --- <समरताताची>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "12\n",
      "चन्नबसवानंद --- <चन्नानाववाद>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "13\n",
      "पियाज्ज़ा --- <पियजज़ा>ा>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "14\n",
      "फैब्रिका --- <फैब्रिका>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "15\n",
      "भावमूर्ति --- <भावमुर्ति>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print('................')\n",
    "    print(i)\n",
    "    print(word_from_torchies(a['output'][i],index_to_hindi_alphabet),\\\n",
    "         '---',\\\n",
    "         make_wrd(bya.transpose(0,1).argmax(2)[i])\\\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word (self, source_batch,target_batch):\n",
    "    max_len, batch_size = target_batch.shape\n",
    "    outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "\n",
    "            # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "    hidden, cell = self.encoder(source_batch)\n",
    "\n",
    "    wordet=[]\n",
    "\n",
    "\n",
    "    trg = torch.tensor(hindi_alphabet_to_index['<'])\n",
    "    trg=trg.to(device)\n",
    "    wordet.append(index_to_hindi_alphabet(trg.cpu().numpy()))\n",
    "    for i in range(1, max_len):\n",
    "        prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "        outputs[i] = prediction\n",
    "        trg = prediction.argmax(1)\n",
    "        wordet.append(index_to_hindi_alphabet(trg.cpu().numpy()))\n",
    "\n",
    "\n",
    "    return ''.join(wordet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_from_vecs(X_valid[0],index_to_english_alphabet,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_from_vecs(y_valid[0],index_to_hindi_alphabet,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc553c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653811",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
