{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aef9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60a160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_english_alphabet=pickle.load(open('vocab_tools/index_to_english_alphabet.pickle', 'rb'))\n",
    "index_to_hindi_alphabet=pickle.load(open('vocab_tools/index_to_hindi_alphabet.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bc19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_alphabet_to_index=pickle.load(open('vocab_tools/hindi_alphabet_to_index.pickle', 'rb')) \n",
    "english_alphabet_to_index=pickle.load(open('vocab_tools/english_alphabet_to_index.pickle', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('simple_data/X_train.npy')\n",
    "X_valid=np.load('simple_data/X_val.npy')\n",
    "\n",
    "y_train=np.load('simple_data/y_train.npy')\n",
    "y_valid=np.load('simple_data/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bb32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e291758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eng_Hind_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, in_file, out_file, root_dir='simple_data',device='cuda'):\n",
    "\n",
    "        self.input = torch.tensor(np.load(root_dir+'/'+in_file))\n",
    "        self.output = torch.tensor(np.load(root_dir+'/'+out_file))\n",
    "        \n",
    "        assert(len(self.input)==len(self.output),\"Error: I/O Lengths must be same\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        X=self.input[idx]\n",
    "        X=X.to(device)\n",
    "        y=self.output[idx]\n",
    "        y=y.to(device)\n",
    "        \n",
    "\n",
    "\n",
    "        sample = {'input': X, 'output': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2d17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d715496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=Eng_Hind_Dataset(\"X_train.npy\",\"y_train.npy\",device=device)\n",
    "val_data=Eng_Hind_Dataset(\"X_val.npy\",\"y_val.npy\",device=device)\n",
    "test_data=Eng_Hind_Dataset(\"X_test.npy\",\"y_test.npy\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9249f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_sets = torch.utils.data.ConcatDataset([training_data, val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b576ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataloader = DataLoader(train_val_sets, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53944bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d97754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e383a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_type(mode:str='rnn'):\n",
    "    mode=mode.lower()\n",
    "    if mode == 'rnn':\n",
    "        return nn.RNN\n",
    "    elif mode =='gru':\n",
    "        return nn.GRU\n",
    "    else:\n",
    "        return nn.LSTM\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e094b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_dim : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, enc_embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, enc_embed_size,padding_idx=english_alphabet_to_index['.'])\n",
    "\n",
    "        #creating LSTM/GRU/RNN cell\n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(enc_embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_batch) # [sent len, batch size, emb dim]\n",
    "        #print('encoder embd',embedded.shape)\n",
    "        \n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            outputs, (hidden, cell) = self.cell(embedded)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded)\n",
    "            cell=outputs\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761debd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, output_size, dec_embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, dec_embed_size,padding_idx=hindi_alphabet_to_index['.'])\n",
    "        \n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(dec_embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        if is_bi:\n",
    "            self.out = nn.Linear(hid_size*2, output_size)\n",
    "        else:\n",
    "             self.out = nn.Linear(hid_size, output_size)\n",
    "        \n",
    "        self.output_size=output_size\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "\n",
    "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trg : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(trg.unsqueeze(1))\n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            #print('decoder embed',embedded.shape)\n",
    "            outputs, (hidden, cell) = self.cell(embedded, (hidden, cell))\n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded, hidden)\n",
    "            cell=hidden\n",
    "        prediction = self.out(outputs.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e441d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, source_batch, target_batch, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        batch_size,max_len  = target_batch.shape\n",
    "        #print(max_len,batch_size)\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        #print(target_vocab_size)\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(source_batch)       \n",
    "\n",
    "        trg = target_batch[:,0]\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if np.random.random() < teacher_forcing_ratio:\n",
    "                trg = target_batch[:,i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a7bf25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_alphabet_to_index['>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a3bb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E=Encoder(30, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi)\n",
    "# E=E.to(device)\n",
    "\n",
    "# D=Decoder(68, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi)\n",
    "\n",
    "# D=D.to(device)\n",
    "# S=Seq2Seq(E,D,device)\n",
    "# S.to(device)    \n",
    "# print(f'The model has {count_params(S):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff54c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30, 64, padding_idx=2)\n",
       "    (cell): LSTM(64, 256, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(68, 128, padding_idx=2)\n",
       "    (cell): LSTM(128, 256, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "    (out): Linear(in_features=512, out_features=68, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=Encoder(30, 64, 256, 1, 'lstm', 0.1, True)\n",
    "E=E.to(device)\n",
    "\n",
    "D=Decoder(68, 128, 256, 1, 'lstm', 0.1, True)\n",
    "\n",
    "D=D.to(device)\n",
    "S=Seq2Seq(E,D,device)\n",
    "S.to(device)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f6133ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc(target_seq,seq2,mode='full',device=device):# predicted\n",
    "    eos_index=(target_seq==hindi_alphabet_to_index['>']).nonzero()\n",
    "    eos_idx=eos_index[:,1]\n",
    "    \n",
    "    correct=torch.Tensor([0]).to(device)\n",
    "    correct_chars=torch.Tensor([0]).to(device)\n",
    "    tot_chars=torch.Tensor([0]).to(device)\n",
    "    for iterate,idx in enumerate(eos_idx):\n",
    "        inputter=seq2[iterate][:idx]\n",
    "        outputter=target_seq[iterate][:idx]\n",
    "        if torch.all(torch.eq(inputter,outputter)):\n",
    "            correct+=1\n",
    "            correct_chars+=idx\n",
    "            tot_chars+=idx\n",
    "        else:\n",
    "            correct_chars+=torch.sum(inputter == outputter).item()\n",
    "            tot_chars+=idx\n",
    "            \n",
    "#         print(correct,correct_chars,tot_chars)\n",
    "        \n",
    "    return correct.item(),correct_chars.item(),tot_chars.item()\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87facdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    seq2seq.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    \n",
    "    relax_acc=0\n",
    "    \n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch['input'], batch['output'])\n",
    "        batch_label=batch['output'].transpose(0,1)\n",
    "        batch_size=len(batch['output'])\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "        outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch_label.reshape(-1)\n",
    "        \n",
    "\n",
    "        trg_flatten.requires_grad=False\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "        correct_temp,correct_chars_temp,tot_chars_temp=accuracy_calc(batch['output'],predicted.transpose(0,1))\n",
    "        \n",
    "        #___________\n",
    "        \n",
    "        correct+=correct_temp\n",
    "        correct_char+=correct_chars_temp\n",
    "        tot_char+=tot_chars_temp\n",
    "        \n",
    "        \n",
    "        #_______________\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "    return epoch_loss / len(iterator), correct/(len(iterator)*16),correct_char/tot_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a950559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    \n",
    "    relax_acc=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            outputs = seq2seq(batch['input'], batch['output'],teacher_forcing_ratio=0)\n",
    "            batch_label=batch['output'].transpose(0,1)\n",
    "            batch_size=len(batch['output'])\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            #print('wow_preds',predicted.shape)\n",
    "\n",
    "            outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch_label.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            \n",
    "            correct_temp,correct_chars_temp,tot_chars_temp=accuracy_calc(batch['output'],predicted.transpose(0,1))\n",
    "        \n",
    "            #___________\n",
    "\n",
    "            correct+=correct_temp\n",
    "            correct_char+=correct_chars_temp\n",
    "            tot_char+=tot_chars_temp\n",
    "\n",
    "            #_______________       \n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "\n",
    "    return epoch_loss / len(iterator), correct/(len(iterator)*16),correct_char/tot_char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "718d4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    e_time = end_time - start_time\n",
    "    mins = e_time // 60\n",
    "    secs = e_time%60\n",
    "    return mins, secs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eb08c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d74d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd5bae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_model(train_iterator,valid_iterator, enc_embed_size,dec_embed_size,\n",
    "               hid_size, num_layers, cell_mode, dropout, is_bi, epochs=20):\n",
    "    E=Encoder(30, enc_embed_size, hid_size, num_layers, cell_mode, dropout, is_bi)\n",
    "    E=E.to(device)\n",
    "    \n",
    "    D=Decoder(68, dec_embed_size, hid_size, num_layers, cell_mode, dropout, is_bi)\n",
    "    \n",
    "    D=D.to(device)\n",
    "    S=Seq2Seq(E,D,device)\n",
    "    S.to(device)    \n",
    "    print(f'The model has {count_params(S):,} trainable parameters')\n",
    "    \n",
    "    optimizer = optim.Adam(S.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=hindi_alphabet_to_index['.'])\n",
    "    criterion=criterion.to(device)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):    \n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc,train_stuff = train(S, train_iterator, optimizer, criterion)\n",
    "        valid_loss,valid_acc,val_stuff = evaluate(S, valid_iterator, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(S.state_dict(), 'model1.pt')\n",
    "\n",
    "        # it's easier to see a change in perplexity between epoch as it's an exponential\n",
    "        # of the loss, hence the scale of the measure is much bigger\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        print(f'\\t Relaxed Train. Acc: {train_stuff*100:.2f}% | Relaxed Val. Acc: {val_stuff*100:.2f}%')\n",
    "#         wandb.log({'epoch':epoch, 'train loss':train_loss, 'train acc':train_acc, 'valid loss': valid_loss,\n",
    "#                   'valid acc': valid_acc, 'relxd train acc': train_stuff, 'relxd valid acc': val_stuff})\n",
    "        \n",
    "    return S\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f239f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iterator=train_val_dataloader\n",
    "# valid_iterator=test_dataloader\n",
    "# SS=make_model(train_iterator,valid_iterator,enc_embed_size=128, dec_embed_size=128,\n",
    "#                hid_size=256, num_layers=3, cell_mode='lstm', dropout=0.3, is_bi=True, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae99ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The second saves and loads the entire model:\n",
    "\n",
    "# torch.save(SS, 'noattn_model.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a446395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then later:\n",
    "\n",
    "the_model = torch.load('noattn_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "452339cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_dataloader = DataLoader(test_data, batch_size=len(test_data),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e79ef338",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model.eval()\n",
    "preds=the_model(next(iter(test_full_dataloader))['input'],next(iter(test_full_dataloader))['output'],teacher_forcing_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a357724",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(preds, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f74d0fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [29, 35, 21,  ..., 47, 25, 18],\n",
       "        [ 4, 20,  6,  ...,  9,  4,  6],\n",
       "        ...,\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea89dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb3f823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_torchies(torchie1,index_toalp):\n",
    "    torchie=torchie1.cpu().numpy()\n",
    "    return word_from_vecs(torchie,index_toalp,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b064da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_batch(batch):\n",
    "    wordlet=[]\n",
    "    for i in range(len(batch)):\n",
    "        wordlet.append(word_from_torchies(batch[i],index_to_hindi_alphabet))\n",
    "    return wordlet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8d3d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_batch_eng(batch):\n",
    "    wordlet=[]\n",
    "    for i in range(len(batch)):\n",
    "        wordlet.append(word_from_torchies(batch[i],index_to_english_alphabet))\n",
    "    return wordlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8373ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds=word_from_batch(predicted.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc070d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41074481, 5.97655678, 7.85201465])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(accuracy_calc(next(iter(test_full_dataloader))['output'],predicted.transpose(0,1)))/len(next(iter(test_full_dataloader))['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d86d544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual=word_from_batch(next(iter(test_full_dataloader))['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "133d2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=word_from_batch_eng(next(iter(test_full_dataloader))['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eec9aded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sikhaaega',\n",
       " 'learn',\n",
       " 'twitters',\n",
       " 'tirunelveli',\n",
       " 'independence',\n",
       " 'speshiyon',\n",
       " 'shurooh',\n",
       " 'kolhapur',\n",
       " 'ajhar',\n",
       " 'karaar',\n",
       " 'anka',\n",
       " 'wpd',\n",
       " 'haashie',\n",
       " 'glendale',\n",
       " 'udhed',\n",
       " 'ekthi',\n",
       " 'idea',\n",
       " 'ambikapur',\n",
       " 'makerere',\n",
       " 'saboodaane',\n",
       " 'foohadta',\n",
       " 'sequent',\n",
       " 'shueb',\n",
       " 'panihati',\n",
       " 'sametati',\n",
       " 'ukhrul',\n",
       " 'brahmlin',\n",
       " 'utaraadhikaaree',\n",
       " 'iqbal',\n",
       " 'dayaalapuraa',\n",
       " 'sohrai',\n",
       " 'takreeban',\n",
       " 'farrukhnagar',\n",
       " 'theinga',\n",
       " 'tyoiharon',\n",
       " 'karneshvardhaam',\n",
       " 'umanath',\n",
       " 'daanshil',\n",
       " 'saahityotsav',\n",
       " 'shantiniketan',\n",
       " 'shikayatkarta',\n",
       " 'andarkhane',\n",
       " 'panter',\n",
       " 'leedaron',\n",
       " 'galgand',\n",
       " 'kaarniyaan',\n",
       " 'murgipaalan',\n",
       " 'mushahid',\n",
       " 'modules',\n",
       " 'rajouri',\n",
       " 'sushrushaa',\n",
       " 'shringaar',\n",
       " 'holt',\n",
       " 'laigikata',\n",
       " 'ijaajat',\n",
       " 'vankshetra',\n",
       " 'bhutal',\n",
       " 'swaadpremiyon',\n",
       " 'nineteez',\n",
       " 'frektar',\n",
       " 'likhkar',\n",
       " 'eyarkandeeshnar',\n",
       " 'nabz',\n",
       " 'quess',\n",
       " 'bouni',\n",
       " 'kaaragujaariyaan',\n",
       " 'gaangnam',\n",
       " 'tapia',\n",
       " 'tezpur',\n",
       " 'talve',\n",
       " 'seemaai',\n",
       " 'darshnaarthi',\n",
       " 'rivas',\n",
       " 'tarkvaad',\n",
       " 'anusaarakaa',\n",
       " 'coachella',\n",
       " 'latakakar',\n",
       " 'patravaliyan',\n",
       " 'parishad',\n",
       " 'spinj',\n",
       " 'anshida',\n",
       " 'dejesus',\n",
       " 'saraaymohiuddinpur',\n",
       " 'lowell',\n",
       " 'capacitor',\n",
       " 'passengerjind',\n",
       " 'granthiyon',\n",
       " 'buena',\n",
       " 'canterbury',\n",
       " 'kaathiyavadi',\n",
       " 'tekchandani',\n",
       " 'fisad',\n",
       " 'beraharamee',\n",
       " 'nishkarshah',\n",
       " 'activities',\n",
       " 'rikailleebreshan',\n",
       " 'shasanadhikaariyon',\n",
       " 'fijoolkharchi',\n",
       " 'dablyoopeedee',\n",
       " 'pace',\n",
       " 'dastar',\n",
       " 'catlin',\n",
       " 'joddta',\n",
       " 'killat',\n",
       " 'gruhnagar',\n",
       " 'wonder',\n",
       " 'vatar',\n",
       " 'shving',\n",
       " 'pashtun',\n",
       " 'farm',\n",
       " 'dibanu',\n",
       " 'pashchamee',\n",
       " 'uthapatak',\n",
       " 'nilaabh',\n",
       " 'laxmeeniyaan',\n",
       " 'mahaanagarawaasiyon',\n",
       " 'upneta',\n",
       " 'convention',\n",
       " 'sharp',\n",
       " 'kaaryayojana',\n",
       " 'maas',\n",
       " 'westing',\n",
       " 'kurvetee',\n",
       " 'jyaada',\n",
       " 'mukeshvari',\n",
       " 'shrimati',\n",
       " 'vivekadhikaron',\n",
       " 'loksabhavalee',\n",
       " 'kabahaa',\n",
       " 'bhraantiyon',\n",
       " 'vivekahin',\n",
       " 'balmiki',\n",
       " 'haryana',\n",
       " 'jivraj',\n",
       " 'flynn',\n",
       " 'rana',\n",
       " 'vishnupura',\n",
       " 'ghotaalebaajon',\n",
       " 'hairatangenz',\n",
       " 'takaleephadeh',\n",
       " 'peepaadh',\n",
       " 'dhabhaashaaraa',\n",
       " 'antdiyan',\n",
       " 'robin',\n",
       " 'singar',\n",
       " 'gumshudagi',\n",
       " 'balkrishna',\n",
       " 'phabti',\n",
       " 'palatne',\n",
       " 'lahlahaati',\n",
       " 'nagaada',\n",
       " 'udanen',\n",
       " 'klein',\n",
       " 'juloos',\n",
       " 'karyabhar',\n",
       " 'manto',\n",
       " 'paimaish',\n",
       " 'covina',\n",
       " 'penshan',\n",
       " 'ogastaavestalaind',\n",
       " 'centreeng',\n",
       " 'activa',\n",
       " 'barker',\n",
       " 'valkan',\n",
       " 'vruton',\n",
       " 'kabaddi',\n",
       " 'raunakh',\n",
       " 'caitlyn',\n",
       " 'phoos',\n",
       " 'jangadnaa',\n",
       " 'upakhyaanon',\n",
       " 'sundram',\n",
       " 'cochin',\n",
       " 'neko',\n",
       " 'kaushal',\n",
       " 'phaliya',\n",
       " 'acevedo',\n",
       " 'kottayam',\n",
       " 'dilaaega',\n",
       " 'kurvi',\n",
       " 'saunpengi',\n",
       " 'lekhan',\n",
       " 'gudepu',\n",
       " 'pharase',\n",
       " 'nubiya',\n",
       " 'taja',\n",
       " 'aantrit',\n",
       " 'qazi',\n",
       " 'murgipalan',\n",
       " 'eddy',\n",
       " 'nagine',\n",
       " 'subha',\n",
       " 'luis',\n",
       " 'vijayapuram',\n",
       " 'bataao',\n",
       " 'ochoa',\n",
       " 'science',\n",
       " 'sarkarein',\n",
       " 'bremerton',\n",
       " 'gurupado',\n",
       " 'sanyukt',\n",
       " 'raalod',\n",
       " 'baadhaon',\n",
       " 'gaumutra',\n",
       " 'sabhaen',\n",
       " 'tani',\n",
       " 'delhi',\n",
       " 'suken',\n",
       " 'vineet',\n",
       " 'chimate',\n",
       " 'kikiyana',\n",
       " 'rocket',\n",
       " 'maxwell',\n",
       " 'hippo',\n",
       " 'deepan',\n",
       " 'arymaa',\n",
       " 'bhulata',\n",
       " 'karmaacharee',\n",
       " 'durgapur',\n",
       " 'tribhaashaa',\n",
       " 'soodkhoron',\n",
       " 'prastaavon',\n",
       " 'survin',\n",
       " 'prapti',\n",
       " 'girivaasiyon',\n",
       " 'siducing',\n",
       " 'welwet',\n",
       " 'muqarrar',\n",
       " 'baxter',\n",
       " 'denier',\n",
       " 'cw',\n",
       " 'uthani',\n",
       " 'ghasi',\n",
       " 'ikattha',\n",
       " 'bhupsingh',\n",
       " 'dabochne',\n",
       " 'crosby',\n",
       " 'naameegiraamee',\n",
       " 'hurley',\n",
       " 'jarda',\n",
       " 'liepradhanmantri',\n",
       " 'bhranti',\n",
       " 'agencyyon',\n",
       " 'bago',\n",
       " 'forcee',\n",
       " 'bosaan',\n",
       " 'karki',\n",
       " 'ginate',\n",
       " 'manokaamnaaon',\n",
       " 'sammpattiyaan',\n",
       " 'politheenon',\n",
       " 'mtake',\n",
       " 'parichar',\n",
       " 'badabadee',\n",
       " 'crane',\n",
       " 'laamiyaan',\n",
       " 'ulhasnagar',\n",
       " 'prabhanmantri',\n",
       " 'shiwalapurwa',\n",
       " 'ferth',\n",
       " 'suhagin',\n",
       " 'saraikela',\n",
       " 'romeoville',\n",
       " 'raajsthanvasiyon',\n",
       " 'pratirodhaatmak',\n",
       " 'svarnikaa',\n",
       " 'vyavasthapana',\n",
       " 'tel',\n",
       " 'galatfahmi',\n",
       " 'takeinsider',\n",
       " 'microfiber',\n",
       " 'shreemahapoorn',\n",
       " 'fallujaa',\n",
       " 'donaldson',\n",
       " 'chadhne',\n",
       " 'jeevvigyaaniyon',\n",
       " 'cricketing',\n",
       " 'reduction',\n",
       " 'awaazen',\n",
       " 'wlaad',\n",
       " 'prathmik',\n",
       " 'thenga',\n",
       " 'excel',\n",
       " 'chatakaate',\n",
       " 'kubuddin',\n",
       " 'taaza',\n",
       " 'padani',\n",
       " 'nichaai',\n",
       " 'wausau',\n",
       " 'aalochakon',\n",
       " 'jaljale',\n",
       " 'surat',\n",
       " 'jeevati',\n",
       " 'prahaarak',\n",
       " 'shreesadguru',\n",
       " 'sanchalanon',\n",
       " 'bhal',\n",
       " 'tughlaq',\n",
       " 'uthaapatak',\n",
       " 'hastaantaraneey',\n",
       " 'utsaahawarddhan',\n",
       " 'sangoshtree',\n",
       " 'shaasanadhikariyon',\n",
       " 'mahadhipatiyon',\n",
       " 'fatima',\n",
       " 'fode',\n",
       " 'kantinyoo',\n",
       " 'kaamanaen',\n",
       " 'chutile',\n",
       " 'sankhyaen',\n",
       " 'kranj',\n",
       " 'barsata',\n",
       " 'parikalpit',\n",
       " 'harpal',\n",
       " 'maaoont',\n",
       " 'grover',\n",
       " 'biolozis',\n",
       " 'halisahar',\n",
       " 'angelo',\n",
       " 'janeudharee',\n",
       " 'bichbachaav',\n",
       " 'pokar',\n",
       " 'apvartit',\n",
       " 'aastin',\n",
       " 'pratishthaavaale',\n",
       " 'buenaventura',\n",
       " 'deccan',\n",
       " 'seriw',\n",
       " 'pardaa',\n",
       " 'emile',\n",
       " 'shaili',\n",
       " 'eastvale',\n",
       " 'chhallaa',\n",
       " 'lakshyon',\n",
       " 'hoaf',\n",
       " 'jarurih',\n",
       " 'rahya',\n",
       " 'mahkane',\n",
       " 'devi',\n",
       " 'nielsen',\n",
       " 'bhira',\n",
       " 'chabutare',\n",
       " 'bhushan',\n",
       " 'rukte',\n",
       " 'aamdani',\n",
       " 'talkshow',\n",
       " 'junagadh',\n",
       " 'padhata',\n",
       " 'odhakar',\n",
       " 'broke',\n",
       " 'talaasha',\n",
       " 'parivaarwaale',\n",
       " 'trubyunal',\n",
       " 'brock',\n",
       " 'yallaappaa',\n",
       " 'tasvaron',\n",
       " 'tugalaki',\n",
       " 'pick',\n",
       " 'circulation',\n",
       " 'jvar',\n",
       " 'kanindham',\n",
       " 'kush',\n",
       " 'properfacebuk',\n",
       " 'ashrafi',\n",
       " 'chadhen',\n",
       " 'vangchuk',\n",
       " 'honeywell',\n",
       " 'radiumdharmee',\n",
       " 'ladhege',\n",
       " 'sachan',\n",
       " 'arthshastriyon',\n",
       " 'canton',\n",
       " 'aatmamugdha',\n",
       " 'pontiac',\n",
       " 'lopez',\n",
       " 'ghontanaa',\n",
       " 'ambani',\n",
       " 'maryadaon',\n",
       " 'ratlam',\n",
       " 'hercules',\n",
       " 'akola',\n",
       " 'sakshan',\n",
       " 'notbandeeshuda',\n",
       " 'lagavaatee',\n",
       " 'alive',\n",
       " 'jivant',\n",
       " 'foda',\n",
       " 'fatahee',\n",
       " 'noobiyaa',\n",
       " 'lalmanee',\n",
       " 'faisala',\n",
       " 'krutyaa',\n",
       " 'dhuan',\n",
       " 'ahuja',\n",
       " 'pratipaalpur',\n",
       " 'namkeen',\n",
       " 'manjilen',\n",
       " 'laffaajee',\n",
       " 'juraab',\n",
       " 'fulae',\n",
       " 'castalloy',\n",
       " 'black',\n",
       " 'sooraj',\n",
       " 'bronch',\n",
       " 'avarnataa',\n",
       " 'johari',\n",
       " 'tevree',\n",
       " 'sanders',\n",
       " 'welder',\n",
       " 'eneeyoo',\n",
       " 'bharashtachaariyon',\n",
       " 'laalmanee',\n",
       " 'khaatedaar',\n",
       " 'aazmaanaa',\n",
       " 'aavrittiyon',\n",
       " 'kullawee',\n",
       " 'bahawalpur',\n",
       " 'prathmikta',\n",
       " 'mobile',\n",
       " 'katihar',\n",
       " 'swecha',\n",
       " 'haamidan',\n",
       " 'samdharshni',\n",
       " 'dridh',\n",
       " 'gond',\n",
       " 'wishesh',\n",
       " 'kaabilegour',\n",
       " 'laurense',\n",
       " 'hisham',\n",
       " 'chausinga',\n",
       " 'pafin',\n",
       " 'mate',\n",
       " 'buildcon',\n",
       " 'teekon',\n",
       " 'athak',\n",
       " 'jindagee',\n",
       " 'chayal',\n",
       " 'maap',\n",
       " 'last',\n",
       " 'gajra',\n",
       " 'ratrichar',\n",
       " 'kaathalaa',\n",
       " 'sahuuliyata',\n",
       " 'pattnaik',\n",
       " 'nyayalayeen',\n",
       " 'mukhyamantriyon',\n",
       " 'jeetanewalon',\n",
       " 'swaphoto',\n",
       " 'clifton',\n",
       " 'doda',\n",
       " 'pendulum',\n",
       " 'chipakane',\n",
       " 'nalakasa',\n",
       " 'chhah',\n",
       " 'gond',\n",
       " 'sameekshaakartaa',\n",
       " 'mantripad',\n",
       " 'liberation',\n",
       " 'asamaany',\n",
       " 'chungiyon',\n",
       " 'arica',\n",
       " 'balot',\n",
       " 'thin',\n",
       " 'formic',\n",
       " 'yuddhak',\n",
       " 'vegas',\n",
       " 'las',\n",
       " 'chitranshi',\n",
       " 'avranta',\n",
       " 'wasting',\n",
       " 'vitark',\n",
       " 'mukabaale',\n",
       " 'dharmapoorwee',\n",
       " 'ambaani',\n",
       " 'ibija',\n",
       " 'mahamedia',\n",
       " 'suzlon',\n",
       " 'varnit',\n",
       " 'jhuthlane',\n",
       " 'madurai',\n",
       " 'kaundal',\n",
       " 'lapatta',\n",
       " 'kshin',\n",
       " 'tewaree',\n",
       " 'raichur',\n",
       " 'lindon',\n",
       " 'pulama',\n",
       " 'olraunder',\n",
       " 'garibon',\n",
       " 'callahan',\n",
       " 'wabag',\n",
       " 'kaipdhari',\n",
       " 'atta',\n",
       " 'bareli',\n",
       " 'jing',\n",
       " 'jivan',\n",
       " 'reddy',\n",
       " 'pehni',\n",
       " 'tanu',\n",
       " 'deepika',\n",
       " 'lohati',\n",
       " 'kakkad',\n",
       " 'maulviyon',\n",
       " 'dungarpur',\n",
       " 'zaalaanaa',\n",
       " 'lakme',\n",
       " 'prospectus',\n",
       " 'westinghouse',\n",
       " 'bawara',\n",
       " 'thakkr',\n",
       " 'ichchhaachaaree',\n",
       " 'triya',\n",
       " 'dhavakon',\n",
       " 'damle',\n",
       " 'shravanabelagola',\n",
       " 'nilkamal',\n",
       " 'karmacharee',\n",
       " 'poorwottar',\n",
       " 'lehrayi',\n",
       " 'haito',\n",
       " 'prakaandh',\n",
       " 'khelmantree',\n",
       " 'bhavishya',\n",
       " 'egmor',\n",
       " 'shringar',\n",
       " 'saupengee',\n",
       " 'knoxville',\n",
       " 'bender',\n",
       " 'sundaram',\n",
       " 'pradesh',\n",
       " 'samvedikaran',\n",
       " 'laguna',\n",
       " 'bhisham',\n",
       " 'yemmiganur',\n",
       " 'huhtamaki',\n",
       " 'vadli',\n",
       " 'gail',\n",
       " 'race',\n",
       " 'salazar',\n",
       " 'diphtheria',\n",
       " 'dradh',\n",
       " 'shadadhi',\n",
       " 'prabodhini',\n",
       " 'lokpratinidhiyon',\n",
       " 'jatati',\n",
       " 'mangla',\n",
       " 'berwyn',\n",
       " 'mum',\n",
       " 'udaygadhi',\n",
       " 'akapulko',\n",
       " 'vaidhruti',\n",
       " 'charanbaddh',\n",
       " 'mandadi',\n",
       " 'euclid',\n",
       " 'paarampariktaa',\n",
       " 'hafate',\n",
       " 'sathiya',\n",
       " 'sanghiyataa',\n",
       " 'mukhyaatithiyon',\n",
       " 'vyavasthavirodhi',\n",
       " 'raktadaataaon',\n",
       " 'baramade',\n",
       " 'tarutal',\n",
       " 'kutta',\n",
       " 'jhaadaa',\n",
       " 'kishun',\n",
       " 'schaeffler',\n",
       " 'amrohi',\n",
       " 'raidical',\n",
       " 'jenner',\n",
       " 'varshiya',\n",
       " 'hasino',\n",
       " 'shaavakon',\n",
       " 'rupa',\n",
       " 'swaarthparak',\n",
       " 'weronika',\n",
       " 'webb',\n",
       " 'mahapatra',\n",
       " 'chenkein',\n",
       " 'agra',\n",
       " 'jiyaangkou',\n",
       " 'westwork',\n",
       " 'janaganna',\n",
       " 'pasco',\n",
       " 'jadhav',\n",
       " 'enterprise',\n",
       " 'sanshayon',\n",
       " 'varnavyavasthaavaadiyon',\n",
       " 'upasthiyon',\n",
       " 'mudichu',\n",
       " 'parnstaron',\n",
       " 'ia',\n",
       " 'baila',\n",
       " 'livaal',\n",
       " 'hillsboro',\n",
       " 'rashtriyadhyaksha',\n",
       " 'peedablyootee',\n",
       " 'thousand',\n",
       " 'shaakir',\n",
       " 'salahkaar',\n",
       " 'vishwakarma',\n",
       " 'ridge',\n",
       " 'swaasthyaheenataa',\n",
       " 'rishvatkhor',\n",
       " 'praavinses',\n",
       " 'daftarvaale',\n",
       " 'physician',\n",
       " 'log',\n",
       " 'jodhika',\n",
       " 'sevalaa',\n",
       " 'doobat',\n",
       " 'sipah',\n",
       " 'bharamaa',\n",
       " 'prtigyaa',\n",
       " 'meghwal',\n",
       " 'nandoi',\n",
       " 'aurora',\n",
       " 'formuley',\n",
       " 'durghtna',\n",
       " 'swatantrasingh',\n",
       " 'monterey',\n",
       " 'baalti',\n",
       " 'columbia',\n",
       " 'whittier',\n",
       " 'dukke',\n",
       " 'cubeck',\n",
       " 'chamakna',\n",
       " 'sheelbhag',\n",
       " 'bistagond',\n",
       " 'findlay',\n",
       " 'bapaa',\n",
       " 'dehra',\n",
       " 'aise',\n",
       " 'shrankhala',\n",
       " 'hastaaksharon',\n",
       " 'tirupati',\n",
       " 'mangalmoortih',\n",
       " 'kursiyon',\n",
       " 'mithaiyaan',\n",
       " 'pahno',\n",
       " 'halebid',\n",
       " 'bangsh',\n",
       " 'officially',\n",
       " 'sevai',\n",
       " 'gurjar',\n",
       " 'samanvayaka',\n",
       " 'parnel',\n",
       " 'sunsan',\n",
       " 'cusex',\n",
       " 'madhya',\n",
       " 'yudhrat',\n",
       " 'urbandale',\n",
       " 'briggs',\n",
       " 'error',\n",
       " 'chikhali',\n",
       " 'waan',\n",
       " 'adhikaariyaan',\n",
       " 'roanoke',\n",
       " 'haokip',\n",
       " 'poorvapekshaaon',\n",
       " 'masheenawat',\n",
       " 'kadmo',\n",
       " 'kurami',\n",
       " 'vidyaathyon',\n",
       " 'simtataa',\n",
       " 'sangrur',\n",
       " 'strongsville',\n",
       " 'agvaai',\n",
       " 'jangan',\n",
       " 'nagraj',\n",
       " 'yariyan',\n",
       " 'bhagyeshwar',\n",
       " 'jivati',\n",
       " 'sudkhoron',\n",
       " 'rukwaakar',\n",
       " 'brahmvarta',\n",
       " 'gray',\n",
       " 'bhraanti',\n",
       " 'purooshottamapur',\n",
       " 'teji',\n",
       " 'svaatan',\n",
       " 'mantarimandaliya',\n",
       " 'ashley',\n",
       " 'mathews',\n",
       " 'moolon',\n",
       " 'akzo',\n",
       " 'hamirpur',\n",
       " 'pokhriyal',\n",
       " 'bolateen',\n",
       " 'pratishthavale',\n",
       " 'dabangata',\n",
       " 'dhuriya',\n",
       " 'teap',\n",
       " 'gazi',\n",
       " 'ijajat',\n",
       " 'mejbani',\n",
       " 'toledo',\n",
       " 'mohadhaa',\n",
       " 'putli',\n",
       " 'sanchari',\n",
       " 'taapanee',\n",
       " 'little',\n",
       " 'narnaari',\n",
       " 'dhaniye',\n",
       " 'gadhoonga',\n",
       " 'faadi',\n",
       " 'gammat',\n",
       " 'tuberculosis',\n",
       " 'nagarjuna',\n",
       " 'raajneechi',\n",
       " 'medizensar',\n",
       " 'laphphaajee',\n",
       " 'commerce',\n",
       " 'maathaloo',\n",
       " 'chatagaon',\n",
       " 'hilig',\n",
       " 'isnpector',\n",
       " 'testing',\n",
       " 'naushad',\n",
       " 'falluja',\n",
       " 'pik',\n",
       " 'jiwan',\n",
       " 'katal',\n",
       " 'dablyooeeef',\n",
       " 'besil',\n",
       " 'slidon',\n",
       " 'saindan',\n",
       " 'jeremy',\n",
       " 'gaanv',\n",
       " 'todenge',\n",
       " 'udaan',\n",
       " 'saptkraanti',\n",
       " 'miranda',\n",
       " 'vyapak',\n",
       " 'maddy',\n",
       " 'sabhasadon',\n",
       " 'nauvahan',\n",
       " 'atmaen',\n",
       " 'rosa',\n",
       " 'damaae',\n",
       " 'midwest',\n",
       " 'trelarah',\n",
       " 'marlborough',\n",
       " 'waco',\n",
       " 'vimarshon',\n",
       " 'khan',\n",
       " 'difarard',\n",
       " 'sarsaa',\n",
       " 'hester',\n",
       " 'uttar',\n",
       " 'kasera',\n",
       " 'kuraaane',\n",
       " 'dayitvon',\n",
       " 'chayaniton',\n",
       " 'morales',\n",
       " 'heros',\n",
       " 'premamoolak',\n",
       " 'moran',\n",
       " 'zydus',\n",
       " 'nelson',\n",
       " 'amarpal',\n",
       " 'phatne',\n",
       " 'tinka',\n",
       " 'gaayikaao',\n",
       " 'pakayen',\n",
       " 'bhrashtachariyon',\n",
       " 'chalit',\n",
       " 'affle',\n",
       " 'mod',\n",
       " 'rajnitikaaron',\n",
       " 'arthon',\n",
       " 'gaaligalauj',\n",
       " 'anyonyashritata',\n",
       " 'udaya',\n",
       " 'cutlery',\n",
       " 'pratibandhah',\n",
       " 'neelaabh',\n",
       " 'bilae',\n",
       " 'khullamkhulla',\n",
       " 'swanson',\n",
       " 'anvaahaarya',\n",
       " 'rewari',\n",
       " 'sahkalakar',\n",
       " 'championship',\n",
       " 'bhadkaayaa',\n",
       " 'namdhari',\n",
       " 'alba',\n",
       " 'watanukoollan',\n",
       " 'khaatadhaarak',\n",
       " 'texas',\n",
       " 'special',\n",
       " 'harvansh',\n",
       " 'dinu',\n",
       " 'shaniwar',\n",
       " 'de',\n",
       " 'dabi',\n",
       " 'nahinkarvaaee',\n",
       " 'maarvaad',\n",
       " 'peediyon',\n",
       " 'brigton',\n",
       " 'mnaey',\n",
       " 'ligar',\n",
       " 'dilaega',\n",
       " 'aadhin',\n",
       " 'parwa',\n",
       " 'hempstead',\n",
       " 'mamooli',\n",
       " 'pradeshikaa',\n",
       " 'verification',\n",
       " 'fefada',\n",
       " 'wisangatiyaa',\n",
       " 'kachchhe',\n",
       " 'ghost',\n",
       " 'pipli',\n",
       " 'lovasa',\n",
       " 'helper',\n",
       " 'doobnewaale',\n",
       " 'bakhshane',\n",
       " 'indriyonke',\n",
       " 'barhad',\n",
       " 'risaalee',\n",
       " 'kaksheevati',\n",
       " 'vishvarakt',\n",
       " 'lodi',\n",
       " 'peachtree',\n",
       " 'ghumakkdon',\n",
       " 'ojha',\n",
       " 'ilaaj',\n",
       " 'arunai',\n",
       " 'ayodhyeanath',\n",
       " 'bhautikwadiyon',\n",
       " 'roket',\n",
       " 'nurochemistry',\n",
       " 'coldvel',\n",
       " 'aawaazen',\n",
       " 'mool',\n",
       " 'multiplexon',\n",
       " 'tyoihar',\n",
       " 'rotation',\n",
       " 'belacha',\n",
       " 'sahakalaakar',\n",
       " 'jadit',\n",
       " 'nagaaraa',\n",
       " 'barpaya',\n",
       " 'doongarpur',\n",
       " 'land',\n",
       " 'uchchhrunkhalataa',\n",
       " 'failaiya',\n",
       " 'suchi',\n",
       " 'noon',\n",
       " 'rp',\n",
       " 'shield',\n",
       " 'palanpur',\n",
       " 'electrosteel',\n",
       " 'obscura',\n",
       " 'cigretton',\n",
       " 'dikhaee',\n",
       " 'russell',\n",
       " 'nazarandaaz',\n",
       " 'kualalampoor',\n",
       " 'austin',\n",
       " 'raidcras',\n",
       " 'tcns',\n",
       " 'svaayattshasan',\n",
       " 'mood',\n",
       " 'daulatabad',\n",
       " 'bhulta',\n",
       " 'pratikaara',\n",
       " 'nights',\n",
       " 'nakam',\n",
       " 'shutting',\n",
       " 'raashtra',\n",
       " 'bechkar',\n",
       " 'stone',\n",
       " 'wardha',\n",
       " 'deedablyutee',\n",
       " 'desabathula',\n",
       " 'baapa',\n",
       " 'baghbaan',\n",
       " 'aadeshah',\n",
       " 'udelkar',\n",
       " 'tarko',\n",
       " 'yaaddaasht',\n",
       " 'inhalation',\n",
       " 'safradganj',\n",
       " 'roopak',\n",
       " 'scranton',\n",
       " 'varnavyavasthaawaadiyon',\n",
       " 'shikshakarmiyon',\n",
       " 'khandelwal',\n",
       " 'beverages',\n",
       " 'pidiya',\n",
       " 'nadaf',\n",
       " 'jehe',\n",
       " 'dhahaaenge',\n",
       " 'mastmaula',\n",
       " 'agartala',\n",
       " 'jamayaa',\n",
       " 'pardesi',\n",
       " 'arya',\n",
       " 'velvet',\n",
       " 'aapdaein',\n",
       " 'aagashe',\n",
       " 'achchi',\n",
       " 'udghosh',\n",
       " 'swabhawatah',\n",
       " 'bani',\n",
       " 'raykar',\n",
       " 'pidilite',\n",
       " 'dual',\n",
       " 'bheera',\n",
       " 'silva',\n",
       " 'adhikaran',\n",
       " 'rangkata',\n",
       " 'srichakra',\n",
       " 'gowmutra',\n",
       " 'nichalaa',\n",
       " 'burhanpur',\n",
       " 'sahooliyata',\n",
       " 'labdh',\n",
       " 'poole',\n",
       " 'strasion',\n",
       " 'picking',\n",
       " 'oditar',\n",
       " 'deniar',\n",
       " 'ajit',\n",
       " 'langdon',\n",
       " 'avashist',\n",
       " 'jha',\n",
       " 'prashasnik',\n",
       " 'bossier',\n",
       " 'badhachadhakar',\n",
       " 'giridih',\n",
       " 'bhuj',\n",
       " 'reading',\n",
       " 'fawaare',\n",
       " 'advait',\n",
       " 'rukate',\n",
       " 'anonditaa',\n",
       " 'fontana',\n",
       " 'nyutriyo',\n",
       " 'pailoton',\n",
       " 'jaden',\n",
       " 'boriwali',\n",
       " 'ilaichi',\n",
       " 'alliance',\n",
       " 'dhammaa',\n",
       " 'nahargadh',\n",
       " 'nsiu',\n",
       " 'sapaat',\n",
       " 'lengdan',\n",
       " 'panama',\n",
       " 'putle',\n",
       " 'aasaapaas',\n",
       " 'alwadhi',\n",
       " 'mufalisi',\n",
       " 'sansathaan',\n",
       " 'darshnaarthee',\n",
       " 'lokotsav',\n",
       " 'graahakonne',\n",
       " 'uchchaadhikaareeyo',\n",
       " 'bhav',\n",
       " 'reja',\n",
       " 'motherson',\n",
       " 'kauplaiks',\n",
       " 'aaburod',\n",
       " 'heidelbergceat',\n",
       " 'chimati',\n",
       " 'surfactants',\n",
       " 'mitao',\n",
       " 'kailana',\n",
       " 'tankar',\n",
       " 'odhkar',\n",
       " 'merta',\n",
       " 'megavaal',\n",
       " 'rasyanik',\n",
       " 'soochnah',\n",
       " 'sugamata',\n",
       " 'jockey',\n",
       " 'munaf',\n",
       " 'niswarth',\n",
       " 'kshetradwaaraa',\n",
       " 'mahawar',\n",
       " 'buldhana',\n",
       " 'dhamkata',\n",
       " 'bhusvami',\n",
       " 'vigyan',\n",
       " 'quete',\n",
       " 'bevkufana',\n",
       " 'jajmau',\n",
       " 'beltran',\n",
       " 'vilas',\n",
       " 'makhdumpur',\n",
       " 'kolorado',\n",
       " 'blackburn',\n",
       " 'suniti',\n",
       " 'ghigghee',\n",
       " 'flavonoids',\n",
       " 'chahunmukhi',\n",
       " 'supachye',\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51e2b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datas={'Ground truth':test_actual,'Predictions':test_preds,}\n",
    "df=pd.DataFrame(data=datas,index=test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07e62ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct=df[df['Ground truth']==df['Predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd5fde00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ground truth</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sikhaaega</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitters</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tirunelveli</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>independence</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speshiyon</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seho</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belcha</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shbana</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>khaatootolaa</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preranapuree</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1648 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ground truth  Predictions\n",
       "sikhaaega               \n",
       "twitters              \n",
       "tirunelveli     \n",
       "independence    \n",
       "speshiyon           \n",
       "...                   ...          ...\n",
       "seho                          \n",
       "belcha                      \n",
       "shbana                      \n",
       "khaatootolaa          \n",
       "preranapuree    \n",
       "\n",
       "[1648 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a885e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incorrect=df[df['Ground truth']!=df['Predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e3e8de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df_incorrect.iloc[-1]['Predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38a8d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(df_incorrect.iloc[-5]['Predictions'],df_incorrect.iloc[-5]['Ground truth']):\n",
    "    print(i==j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85a5039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*df_incorrect.iloc[-5]['Predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2d2e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*df_incorrect.iloc[-5]['Ground truth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e574bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incorrect.iloc[-5]['Ground truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5f3b886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ground truth</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learn</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shurooh</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ajhar</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karaar</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anka</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aphasaron</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chabate</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miti</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saflata</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shivastava</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2447 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Ground truth Predictions\n",
       "learn                      \n",
       "shurooh                  \n",
       "ajhar                       \n",
       "karaar                    \n",
       "anka                      \n",
       "...                 ...         ...\n",
       "aphasaron             \n",
       "chabate                   \n",
       "miti                       \n",
       "saflata                 \n",
       "shivastava        \n",
       "\n",
       "[2447 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42ea6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions_vanilla.txt',test_preds, delimiter=',',encoding='utf-8',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16ef8581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(68, 128, padding_idx=2)\n",
       "  (cell): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (out): Linear(in_features=512, out_features=68, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
