{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd4dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55566c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('aksharantar_sampled/hin/hin_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e916919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=pd.read_csv('aksharantar_sampled/hin/hin_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b70f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('aksharantar_sampled/hin/hin_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ece7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindi_tokenizer(hindi_word):\n",
    "    return list(hindi_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea11264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindi_alphabet_vocab(df):\n",
    "    hindi_vocab=[]\n",
    "    for word in df['शस्त्रागार']:\n",
    "        for alphabet in hindi_tokenizer(word):\n",
    "            if alphabet in hindi_vocab:\n",
    "                pass\n",
    "            else:\n",
    "                hindi_vocab.append(alphabet)\n",
    "            \n",
    "    hindi_vocab.append('<')\n",
    "    hindi_vocab.append('>')\n",
    "    hindi_vocab.append('.')\n",
    "    return hindi_vocab\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab17d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_alphabet_vocab(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4554449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_tokenizer(english_word):\n",
    "    return list(english_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8003916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_alphabet_vocab(df):\n",
    "    english_vocab=[]\n",
    "    for word in df['shastragaar']:\n",
    "        for alphabet in english_tokenizer(word):\n",
    "            if alphabet in english_vocab:\n",
    "                pass\n",
    "            else:\n",
    "                english_vocab.append(alphabet)\n",
    "            \n",
    "    english_vocab.append('<')\n",
    "    english_vocab.append('>')\n",
    "    english_vocab.append('.')\n",
    "    return english_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03b0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(do_onehot=True):\n",
    "    'returns encoder function'\n",
    "    def one_hot_encode_alphabet(alphabet,alphabet_to_index):\n",
    "    \n",
    "        # Initialize a zero vector of length equal to the number of alphabets plus one for unseen alphabets\n",
    "        one_hot_vector = [0] * (len(alphabet_to_index) + 1)\n",
    "        # Set the indices of the alphabets present in the sentence to 1\n",
    "\n",
    "        if alphabet.lower() in alphabet_to_index:\n",
    "            one_hot_vector[alphabet_to_index[alphabet.lower()]] = 1\n",
    "        else:\n",
    "            # Set the last index to 1 to represent any unseen alphabets\n",
    "            one_hot_vector[-1] = 1\n",
    "        return one_hot_vector\n",
    "    def simple(alphabet,alphabet_to_index):\n",
    "        try:\n",
    "            return alphabet_to_index[alphabet.lower()]\n",
    "        except:\n",
    "            return len(alphabet_to_index)\n",
    "    \n",
    "    \n",
    "    if do_onehot:\n",
    "        return one_hot_encode_alphabet\n",
    "    else:\n",
    "        return simple\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff769d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word,alphabet_to_index,encoder=encode()):\n",
    "    max_len=30\n",
    "    word='<'+word+'>'\n",
    "    \n",
    "    while len(word)<30:\n",
    "        word+='.'\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_encode=[]\n",
    "    for letter in word:\n",
    "        word_encode.append(encoder(letter,alphabet_to_index))\n",
    "    \n",
    "    return word_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec094743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28706e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(df_train,df_val=None,df_test=None,do_onehot=True):\n",
    "    # we generate the vocabulary from train and validation only!\n",
    "    hindi_vocab=hindi_alphabet_vocab(df_train)\n",
    "    english_vocab=english_alphabet_vocab(df_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    vocab generated. Now we one hot them?\n",
    "    yes-> because one hot encoding will ensure that distances don't have meaning 1,2 for a,b\n",
    "    and 26 for z would imply b is closer to a\n",
    "    '''\n",
    "    \n",
    "    # Create a dictionary to map alphabets to their index\n",
    "    \n",
    "\n",
    "    \n",
    "    hindi_alphabet_to_index = {alphabet: index for index, alphabet in enumerate(hindi_vocab)}\n",
    "    index_to_hindi_alphabet = {index:alphabet  for index, alphabet in enumerate(hindi_vocab)}\n",
    "    \n",
    "    english_alphabet_to_index = {alphabet: index for index, alphabet in enumerate(english_vocab)}\n",
    "    index_to_english_alphabet = {index:alphabet  for index, alphabet in enumerate(english_vocab)}\n",
    "\n",
    "    if do_onehot:\n",
    "        with open('vocab_tools/hindi_alphabet_to_index.pickle', 'wb') as f:\n",
    "            pickle.dump(hindi_alphabet_to_index, f)\n",
    "        with open('vocab_tools/index_to_hindi_alphabet.pickle', 'wb') as f:\n",
    "            pickle.dump(index_to_hindi_alphabet, f)\n",
    "        with open('vocab_tools/english_alphabet_to_index.pickle', 'wb') as f:\n",
    "            pickle.dump(english_alphabet_to_index, f)\n",
    "        with open('vocab_tools/index_to_english_alphabet.pickle', 'wb') as f:\n",
    "            pickle.dump(index_to_english_alphabet, f)\n",
    "    \n",
    "    #training_data\n",
    "    training_input=[]\n",
    "    training_output=[]\n",
    "    \n",
    "    #validation\n",
    "    valid_input=[]\n",
    "    valid_output=[]\n",
    "    \n",
    "    #test\n",
    "    test_input=[]\n",
    "    test_output=[]\n",
    "    \n",
    "    for eng_word,hindi_word in zip(df_train['shastragaar'],df_train['शस्त्रागार']):\n",
    "        training_input.append(encode_word(eng_word,english_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "        training_output.append(encode_word(hindi_word,hindi_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "\n",
    "        \n",
    "    if df_val is not None:\n",
    "        for eng_word,hindi_word in zip(df_val['jaisawal'],df_val['जयसवाल']):\n",
    "            valid_input.append(encode_word(eng_word,english_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "            valid_output.append(encode_word(hindi_word,hindi_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "\n",
    "        \n",
    "    if df_test is not None:\n",
    "        for eng_word,hindi_word in zip(df_test['thermax'],df_test['थरमैक्स']):\n",
    "            test_input.append(encode_word(eng_word,english_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "            test_output.append(encode_word(hindi_word,hindi_alphabet_to_index,encoder=encode(do_onehot)))\n",
    "\n",
    "    \n",
    "    if do_onehot:\n",
    "        np.save('one_hot_data/X_train.npy',training_input)\n",
    "        np.save('one_hot_data/y_train.npy',training_output)\n",
    "        np.save('one_hot_data/X_val.npy',valid_input)\n",
    "        np.save('one_hot_data/y_val.npy',valid_output)\n",
    "        np.save('one_hot_data/X_test.npy',test_input)\n",
    "        np.save('one_hot_data/y_test.npy',test_output)\n",
    "    else:\n",
    "        np.save('simple_data/X_train.npy',training_input)\n",
    "        np.save('simple_data/y_train.npy',training_output)\n",
    "        np.save('simple_data/X_val.npy',valid_input)\n",
    "        np.save('simple_data/y_val.npy',valid_output)\n",
    "        np.save('simple_data/X_test.npy',test_input)\n",
    "        np.save('simple_data/y_test.npy',test_output)        \n",
    "    \n",
    "    return\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f397bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet_from_encode(one_hot_vector_or_index, index_to_alphabet, do_onehot=True):\n",
    "    \n",
    "    if do_onehot:\n",
    "        index = one_hot_vector_or_index.index(1)\n",
    "    else:\n",
    "        index = one_hot_vector_or_index\n",
    "    \n",
    "    try:\n",
    "        alphabet = index_to_alphabet[index]\n",
    "        return alphabet\n",
    "    except:\n",
    "        alphabet=' '\n",
    "        return alphabet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f36610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_vecs(vecs,index_to_alphabet, do_onehot=True):\n",
    "    \n",
    "    if get_alphabet_from_encode(vecs[0],index_to_alphabet, do_onehot)!='<':\n",
    "        print(\"Invalid Word\")\n",
    "    else:\n",
    "        word=[]\n",
    "        for ij in range(1,len(vecs)):\n",
    "            aphab=get_alphabet_from_encode(vecs[ij],index_to_alphabet, do_onehot)\n",
    "            if aphab=='>':\n",
    "                return ''.join(word)\n",
    "            word.append(aphab)\n",
    "        return word\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc669c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator(df_train,df_valid,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0805ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator(df_train,df_valid,df_test,do_onehot=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
