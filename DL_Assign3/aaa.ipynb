{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aef9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60a160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_english_alphabet=pickle.load(open('vocab_tools/index_to_english_alphabet.pickle', 'rb'))\n",
    "index_to_hindi_alphabet=pickle.load(open('vocab_tools/index_to_hindi_alphabet.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bc19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_alphabet_to_index=pickle.load(open('vocab_tools/hindi_alphabet_to_index.pickle', 'rb')) \n",
    "english_alphabet_to_index=pickle.load(open('vocab_tools/english_alphabet_to_index.pickle', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('simple_data/X_train.npy')\n",
    "X_valid=np.load('simple_data/X_val.npy')\n",
    "\n",
    "y_train=np.load('simple_data/y_train.npy')\n",
    "y_valid=np.load('simple_data/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bb32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/shared_conda_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e291758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eng_Hind_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, in_file, out_file, root_dir='simple_data',device='cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.input = torch.tensor(np.load(root_dir+'/'+in_file))\n",
    "        self.output = torch.tensor(np.load(root_dir+'/'+out_file))\n",
    "        \n",
    "        assert(len(self.input)==len(self.output),\"Error: I/O Lengths must be same\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        X=self.input[idx]\n",
    "        X=X.to(device)\n",
    "        y=self.output[idx]\n",
    "        y=y.to(device)\n",
    "        \n",
    "\n",
    "\n",
    "        sample = {'input': X, 'output': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2d17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d715496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=Eng_Hind_Dataset(\"X_train.npy\",\"y_train.npy\",device=device)\n",
    "val_data=Eng_Hind_Dataset(\"X_val.npy\",\"y_val.npy\",device=device)\n",
    "test_data=Eng_Hind_Dataset(\"X_test.npy\",\"y_test.npy\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b576ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53944bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_data, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77a6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0663214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d97754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e383a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_type(mode:str='rnn'):\n",
    "    mode=mode.lower()\n",
    "    if mode == 'rnn':\n",
    "        return nn.RNN\n",
    "    elif mode =='gru':\n",
    "        return nn.GRU\n",
    "    else:\n",
    "        return nn.LSTM\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e094b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_dim : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, embed_size,padding_idx=english_alphabet_to_index['.'])\n",
    "\n",
    "        #creating LSTM/GRU/RNN cell\n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_batch) # [sent len, batch size, emb dim]\n",
    "        #print('encoder embd',embedded.shape)\n",
    "        \n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            outputs, (hidden, cell) = self.cell(embedded)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded)\n",
    "            cell=outputs\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761debd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, output_size, embed_size, hid_size, num_layers, cell_mode, dropout, is_bi):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embed_size,padding_idx=hindi_alphabet_to_index['.'])\n",
    "        \n",
    "        cell=cell_type(cell_mode)\n",
    "        \n",
    "        self.cell=cell(embed_size,hid_size,num_layers,dropout=dropout,bidirectional=is_bi,batch_first=True)\n",
    "        self.out = nn.Linear(hid_size, output_size)\n",
    "        \n",
    "        self.output_size=output_size\n",
    "        self.cell_mode=cell_mode\n",
    "        \n",
    "\n",
    "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trg : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(trg.unsqueeze(1))\n",
    "        if self.cell_mode.lower()=='lstm':\n",
    "            #print('decoder embed',embedded.shape)\n",
    "            outputs, (hidden, cell) = self.cell(embedded, (hidden, cell))\n",
    "        else:\n",
    "            outputs, hidden = self.cell(embedded, hidden)\n",
    "            cell=hidden\n",
    "        prediction = self.out(outputs.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e441d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, source_batch, target_batch, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        batch_size,max_len  = target_batch.shape\n",
    "        #print(max_len,batch_size)\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        #print(target_vocab_size)\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(source_batch)       \n",
    "\n",
    "        trg = target_batch[:,0]\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = target_batch[:,i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a7bf25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_alphabet_to_index['>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcad189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss1=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b07619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[ 0, 12, 17, 11, 11,  9, 20,  8, 18,  4,  5,  1,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  5,  4,  5,  8,  9,  5, 16, 17,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  6,  8,  5,  9,  3, 14, 23,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 19,  9, 20,  9,  6,  7,  9,  5, 23,  9,  1,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  9, 20,  4, 11, 27,  9,  6, 14,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  3,  9, 20,  3,  7,  9,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 12,  4, 11, 23, 14, 10,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  9,  5,  9,  9,  6,  4, 12, 16,  9,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 12, 11,  4, 23,  4,  5, 17,  9, 11,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  6,  7,  9,  5, 20, 17,  4,  5,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  6, 17, 13,  9,  5,  4,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 21, 15,  9, 11,  9, 24,  9,  8, 15,  9, 23,  1,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  9, 19,  7, 22,  9, 19,  5,  9, 11, 12,  7,  1,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 20,  9,  5,  6,  9, 10, 19,  7,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 24,  9, 23, 16,  4, 13,  8,  9,  5,  4,  8,  9,  1,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  5, 14, 21, 10, 11,  4,  8,  9,  5,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2]], device='cuda:0'),\n",
       " 'output': tensor([[ 0, 21, 24, 11,  9, 27,  9, 37, 29, 20,  5,  1,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  5,  4,  5,  6,  8,  9,  5, 19, 24,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 32,  9,  8,  5,  9,  3, 17, 35,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 29, 27,  9, 28,  9,  5, 35,  9,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 40, 27, 20, 11, 15,  9,  7, 17,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  3,  9, 13, 26,  9,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 14, 11, 35, 17, 10,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 40,  5,  9,  7,  4, 14,  6, 19,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 21,  6, 11,  9, 20, 44, 35,  4,  5,  4,  8, 11,  1,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 28,  5, 27, 24, 13,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  7, 24, 22,  5, 20,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 49, 18, 11,  9, 15,  8, 18,  9, 35,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 43, 25,  6, 19,  9, 29,  5,  9, 11,  6, 50,  1,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 27, 13,  7,  9, 10,  6, 48,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0, 15, 35, 19,  4, 15,  6, 16,  9,  5, 20,  8,  1,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 0,  5, 42, 10, 11,  4,  8,  9, 53,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f6133ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc(target_seq,seq2,mode='full',device=device):# predicted\n",
    "    eos_index=(target_seq==hindi_alphabet_to_index['>']).nonzero()\n",
    "    eos_idx=eos_index[:,1]\n",
    "    \n",
    "    correct=torch.Tensor([0]).to(device)\n",
    "    correct_chars=torch.Tensor([0]).to(device)\n",
    "    tot_chars=torch.Tensor([0]).to(device)\n",
    "    for iterate,idx in enumerate(eos_idx):\n",
    "        inputter=seq2[iterate][:idx]\n",
    "        outputter=target_seq[iterate][:idx]\n",
    "        if torch.all(torch.eq(inputter,outputter)):\n",
    "            correct+=1\n",
    "            correct_chars+=idx\n",
    "            tot_chars+=idx\n",
    "        else:\n",
    "            correct_chars+=torch.sum(inputter == outputter).item()\n",
    "            tot_chars+=idx\n",
    "            \n",
    "#         print(correct,correct_chars,tot_chars)\n",
    "        \n",
    "    return correct.item(),correct_chars.item(),tot_chars.item()\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee69981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.0, 134.0, 134.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calc(ss1['output'],ss1['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87facdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    seq2seq.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    \n",
    "    relax_acc=0\n",
    "    \n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch['input'], batch['output'])\n",
    "        batch_label=batch['output'].transpose(0,1)\n",
    "        batch_size=len(batch['output'])\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "        outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch_label.reshape(-1)\n",
    "        \n",
    "\n",
    "        trg_flatten.requires_grad=False\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "        correct_temp,correct_chars_temp,tot_chars_temp=accuracy_calc(batch['output'],predicted.transpose(0,1))\n",
    "        \n",
    "        #___________\n",
    "        \n",
    "        correct+=correct_temp\n",
    "        correct_char+=correct_chars_temp\n",
    "        tot_char+=tot_chars_temp\n",
    "        \n",
    "        \n",
    "        #_______________\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "    return epoch_loss / len(iterator), correct/(len(iterator)*16),correct_char/tot_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a950559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    correct=0\n",
    "    correct_char=0\n",
    "    tot_char=0\n",
    "    \n",
    "    relax_acc=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            outputs = seq2seq(batch['input'], batch['output'],teacher_forcing_ratio=0)\n",
    "            batch_label=batch['output'].transpose(0,1)\n",
    "            batch_size=len(batch['output'])\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            #print('wow_preds',predicted.shape)\n",
    "\n",
    "            outputs_flatten = outputs.view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch_label.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            \n",
    "            correct_temp,correct_chars_temp,tot_chars_temp=accuracy_calc(batch['output'],predicted.transpose(0,1))\n",
    "        \n",
    "            #___________\n",
    "\n",
    "            correct+=correct_temp\n",
    "            correct_char+=correct_chars_temp\n",
    "            tot_char+=tot_chars_temp\n",
    "\n",
    "            #_______________       \n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "\n",
    "    return epoch_loss / len(iterator), correct/(len(iterator)*16),correct_char/tot_char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "718d4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    e_time = end_time - start_time\n",
    "    mins = e_time // 60\n",
    "    secs = e_time%60\n",
    "    return mins, secs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eb08c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d74d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd5bae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_model(train_iterator,valid_iterator,N_EPOCHS=20):\n",
    "    E=Encoder(30,128,128,2,'lstm',0.27,False)\n",
    "    E=E.to(device)\n",
    "    D=Decoder(68,128,128,2,'lstm',0.27,False)\n",
    "    D=D.to(device)\n",
    "    S=Seq2Seq(E,D,device)\n",
    "    S.to(device)    \n",
    "    print(f'The model has {count_params(S):,} trainable parameters')\n",
    "    \n",
    "    optimizer = optim.Adam(S.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=hindi_alphabet_to_index['.'])\n",
    "    criterion=criterion.to(device)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "#     return S\n",
    "\n",
    "    for epoch in range(N_EPOCHS):    \n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc,train_stuff = train(S, train_iterator, optimizer, criterion)\n",
    "        valid_loss,valid_acc,val_stuff = evaluate(S, valid_iterator, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(S.state_dict(), 'model1.pt')\n",
    "\n",
    "        # it's easier to see a change in perplexity between epoch as it's an exponential\n",
    "        # of the loss, hence the scale of the measure is much bigger\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        print(f'\\t Relaxed Train. Acc: {train_stuff*100:.2f}% | Relaxed Val. Acc: {val_stuff*100:.2f}%')\n",
    "        \n",
    "    return S\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1049df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 549,700 trainable parameters\n",
      "Epoch: 01 | Time: 0.0m 33.43s\n",
      "\t Train Loss: 2.676 | Train Acc: 0.37%\n",
      "\t Val. Loss: 2.045 |  Val. Acc: 7.71%\n",
      "\t Relaxed Train. Acc: 33.89% | Relaxed Val. Acc: 52.78%\n",
      "Epoch: 02 | Time: 0.0m 33.38s\n",
      "\t Train Loss: 1.644 | Train Acc: 6.91%\n",
      "\t Val. Loss: 1.657 |  Val. Acc: 19.09%\n",
      "\t Relaxed Train. Acc: 60.68% | Relaxed Val. Acc: 63.59%\n",
      "Epoch: 03 | Time: 0.0m 33.33s\n",
      "\t Train Loss: 1.393 | Train Acc: 13.36%\n",
      "\t Val. Loss: 1.602 |  Val. Acc: 25.00%\n",
      "\t Relaxed Train. Acc: 68.23% | Relaxed Val. Acc: 66.11%\n",
      "Epoch: 04 | Time: 0.0m 33.29s\n",
      "\t Train Loss: 1.289 | Train Acc: 17.07%\n",
      "\t Val. Loss: 1.529 |  Val. Acc: 26.27%\n",
      "\t Relaxed Train. Acc: 71.49% | Relaxed Val. Acc: 68.20%\n",
      "Epoch: 05 | Time: 0.0m 33.24s\n",
      "\t Train Loss: 1.224 | Train Acc: 19.82%\n",
      "\t Val. Loss: 1.510 |  Val. Acc: 27.93%\n",
      "\t Relaxed Train. Acc: 73.64% | Relaxed Val. Acc: 69.80%\n",
      "Epoch: 06 | Time: 0.0m 33.23s\n",
      "\t Train Loss: 1.173 | Train Acc: 22.12%\n",
      "\t Val. Loss: 1.497 |  Val. Acc: 28.69%\n",
      "\t Relaxed Train. Acc: 75.31% | Relaxed Val. Acc: 70.29%\n",
      "Epoch: 07 | Time: 0.0m 33.20s\n",
      "\t Train Loss: 1.140 | Train Acc: 24.06%\n",
      "\t Val. Loss: 1.454 |  Val. Acc: 31.37%\n",
      "\t Relaxed Train. Acc: 76.45% | Relaxed Val. Acc: 71.50%\n",
      "Epoch: 08 | Time: 0.0m 33.20s\n",
      "\t Train Loss: 1.113 | Train Acc: 25.12%\n",
      "\t Val. Loss: 1.454 |  Val. Acc: 31.64%\n",
      "\t Relaxed Train. Acc: 77.33% | Relaxed Val. Acc: 71.57%\n",
      "Epoch: 09 | Time: 0.0m 33.18s\n",
      "\t Train Loss: 1.092 | Train Acc: 26.63%\n",
      "\t Val. Loss: 1.450 |  Val. Acc: 32.08%\n",
      "\t Relaxed Train. Acc: 78.08% | Relaxed Val. Acc: 72.27%\n",
      "Epoch: 10 | Time: 0.0m 33.17s\n",
      "\t Train Loss: 1.068 | Train Acc: 27.78%\n",
      "\t Val. Loss: 1.441 |  Val. Acc: 33.84%\n",
      "\t Relaxed Train. Acc: 78.83% | Relaxed Val. Acc: 72.58%\n",
      "Epoch: 11 | Time: 0.0m 33.15s\n",
      "\t Train Loss: 1.051 | Train Acc: 28.73%\n",
      "\t Val. Loss: 1.427 |  Val. Acc: 33.72%\n",
      "\t Relaxed Train. Acc: 79.36% | Relaxed Val. Acc: 72.73%\n",
      "Epoch: 12 | Time: 0.0m 33.13s\n",
      "\t Train Loss: 1.034 | Train Acc: 29.80%\n",
      "\t Val. Loss: 1.431 |  Val. Acc: 33.94%\n",
      "\t Relaxed Train. Acc: 79.98% | Relaxed Val. Acc: 73.06%\n",
      "Epoch: 13 | Time: 0.0m 33.12s\n",
      "\t Train Loss: 1.018 | Train Acc: 30.37%\n",
      "\t Val. Loss: 1.415 |  Val. Acc: 35.11%\n",
      "\t Relaxed Train. Acc: 80.43% | Relaxed Val. Acc: 73.20%\n",
      "Epoch: 14 | Time: 0.0m 33.11s\n",
      "\t Train Loss: 1.010 | Train Acc: 31.63%\n",
      "\t Val. Loss: 1.417 |  Val. Acc: 35.55%\n",
      "\t Relaxed Train. Acc: 80.84% | Relaxed Val. Acc: 73.66%\n",
      "Epoch: 15 | Time: 0.0m 33.10s\n",
      "\t Train Loss: 1.001 | Train Acc: 32.03%\n",
      "\t Val. Loss: 1.433 |  Val. Acc: 36.25%\n",
      "\t Relaxed Train. Acc: 81.04% | Relaxed Val. Acc: 73.95%\n"
     ]
    }
   ],
   "source": [
    "train_iterator=train_dataloader\n",
    "valid_iterator=val_dataloader\n",
    "SS=make_model(train_iterator,valid_iterator,N_EPOCHS=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0ac4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0512bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bya=SS(a['input'],a['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b03dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5071c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 16, 68])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bya.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea89dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb3f823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_torchies(torchie1,index_toalp):\n",
    "    torchie=torchie1.cpu().numpy()\n",
    "    return word_from_vecs(torchie,index_toalp,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b064da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_batch(batch):\n",
    "    wordlet=[]\n",
    "    for i in range(len(batch)):\n",
    "        wordlet.append(word_from_torchies(batch[i],index_to_hindi_alphabet))\n",
    "    return wordlet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8373ea7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मेडियाच'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_from_torchies(bya.transpose(0,1).argmax(2)[0],index_to_hindi_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "123a7f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ङ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_hindi_alphabet[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccef40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrd(stuff):\n",
    "    asa=[]\n",
    "    for k in stuff.cpu().numpy():\n",
    "        asa.append(index_to_hindi_alphabet[k])\n",
    "    return \"\".join(asa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a4bd0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<दोशारोपत्र>>>>>>>>>>>ी>>>>>>>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_wrd(bya.transpose(0,1).argmax(2)[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32caa83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  7, 17, 25,  9, 11, 17, 18, 14,  6, 11,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1, 20,  1,  1,  1,  1,  1,  1,  1], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bya.transpose(0,1).argmax(2)[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb2cde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\n",
      "0\n",
      "मीडियाच --- <मेडियाच>>>>>>>आआई>>>>>>>>>>>>\n",
      "................\n",
      "1\n",
      "जशिला --- <जशिला>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "2\n",
      "डिजाइनर्स --- <डिजिइनर्स>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "3\n",
      "लाइसराम --- <लाससराम>>>>>र>>>>>>>>>>>>>>>>\n",
      "................\n",
      "4\n",
      "इच्छेला --- <इछेछेला>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "5\n",
      "गँवाना --- <गंवनना>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "6\n",
      "क्रोधाग्नी --- <क्रोधगग्नी>>ी>ीी>>>>>>>>>>>>>\n",
      "................\n",
      "7\n",
      "रस्सियाँ --- <रसससियाँ>>>>>>>>>>>ीी>>>>>>>>\n",
      "................\n",
      "8\n",
      "सनाचा --- <सााचा>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "9\n",
      "पटरीरहित --- <पताीरहित>>>>ी>>>>>>ीी>>>>>>>>\n",
      "................\n",
      "10\n",
      "दोषारोपत्र --- <दोशारोपत्र>>>>>>>>>>>ी>>>>>>>\n",
      "................\n",
      "11\n",
      "यदविन्दर --- <यद्िं्द्र>>>>>>>>>>ीी>>>>>>>>\n",
      "................\n",
      "12\n",
      "मान्छू --- <मांचचू>>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "13\n",
      "डायमंडचा --- <डायमोडचा>>>>>>>>>>>ीी>>>>>>>>\n",
      "................\n",
      "14\n",
      "शिवारचे --- <शिवारचे>>>>>>>>>>>>>>>>>>>>>>\n",
      "................\n",
      "15\n",
      "स्प्रेस --- <स्प्रेस>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print('................')\n",
    "    print(i)\n",
    "    print(word_from_torchies(a['output'][i],index_to_hindi_alphabet),\\\n",
    "         '---',\\\n",
    "         make_wrd(bya.transpose(0,1).argmax(2)[i])\\\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word (self, source_batch,target_batch):\n",
    "    max_len, batch_size = target_batch.shape\n",
    "    outputs = torch.zeros(max_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "\n",
    "            # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "    hidden, cell = self.encoder(source_batch)\n",
    "\n",
    "    wordet=[]\n",
    "\n",
    "\n",
    "    trg = torch.tensor(hindi_alphabet_to_index['<'])\n",
    "    trg=trg.to(device)\n",
    "    wordet.append(index_to_hindi_alphabet(trg.cpu().numpy()))\n",
    "    for i in range(1, max_len):\n",
    "        prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "        outputs[i] = prediction\n",
    "        trg = prediction.argmax(1)\n",
    "        wordet.append(index_to_hindi_alphabet(trg.cpu().numpy()))\n",
    "\n",
    "\n",
    "    return ''.join(wordet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_from_vecs(X_valid[0],index_to_english_alphabet,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_from_vecs(y_valid[0],index_to_hindi_alphabet,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc553c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b2342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "            for j in range(predicted.shape[1]):\n",
    "                predicted_seq = predicted[:, j]\n",
    "                targets_seq = target_seq[:, j]\n",
    "\n",
    "                # Find the index of the first EOS token in the sequence\n",
    "                eos_idx = (targets_seq == hin_token_map[\"\\n\"]).nonzero()\n",
    "                if eos_idx.numel() > 0:\n",
    "                    eos_idx = eos_idx[0][0]\n",
    "                    predicted_seq = predicted_seq[:eos_idx]\n",
    "                    targets_seq = targets_seq[:eos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653811",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
